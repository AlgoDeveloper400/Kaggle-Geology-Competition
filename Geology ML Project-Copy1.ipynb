{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdda2998-d743-419f-9d8c-9992499cc197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(r\"Your file path to the data file...\")\n",
    "\n",
    "# Display the first 5 rows\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610bcb69-8cb1-4424-9655-de38285e0198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(r\"Your file path to the data file...\")\n",
    "\n",
    "# Exclude 'geology_id' from interpolation\n",
    "columns_to_interpolate = df.columns[df.columns != 'geology_id']\n",
    "\n",
    "# Apply linear interpolation across columns (axis=1)\n",
    "df[columns_to_interpolate] = df[columns_to_interpolate].interpolate(\n",
    "    method='linear', axis=1, limit_direction='both'\n",
    ")\n",
    "\n",
    "# If any NaNs still remain (e.g., all values were NaN in a row), fallback to column mean\n",
    "df[columns_to_interpolate] = df[columns_to_interpolate].fillna(df[columns_to_interpolate].mean())\n",
    "\n",
    "# Define path to save the new CSV\n",
    "save_folder = r\"Your file path to the data file...\"\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "save_path = os.path.join(save_folder, \"filled_train.csv\")\n",
    "\n",
    "# Save the filled DataFrame\n",
    "df.to_csv(save_path, index=False)\n",
    "\n",
    "# Optionally print the first few rows\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d5a3a9-899c-46e5-8d60-e3e40a86ad7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# === Load training data ===\n",
    "train_path = r\"Your file path to the data file...\"\n",
    "df = pd.read_csv(train_path)\n",
    "\n",
    "# === Extract only columns named '-299' to '0' ===\n",
    "input_cols = [str(i) for i in range(-299, 1) if str(i) in df.columns]\n",
    "\n",
    "# Targets are all columns except 'geology_id' and inputs\n",
    "target_cols = [col for col in df.columns if col not in input_cols + ['geology_id']]\n",
    "\n",
    "print(f\"Number of input columns: {len(input_cols)}\")\n",
    "print(f\"Number of target columns: {len(target_cols)}\")\n",
    "\n",
    "# Save target_cols list for inference use\n",
    "target_cols_path = r\"Your file path to the data file...\"\n",
    "with open(target_cols_path, \"wb\") as f:\n",
    "    pickle.dump(target_cols, f)\n",
    "print(f\"Saved target columns list to: {target_cols_path}\")\n",
    "\n",
    "# === Normalize inputs and targets ===\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "X_train = scaler_x.fit_transform(df[input_cols].values.astype(np.float32))\n",
    "Y_train = scaler_y.fit_transform(df[target_cols].values.astype(np.float32))\n",
    "\n",
    "# Save scalers\n",
    "with open(r\"Your file path to the data file...\\scaler_x.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler_x, f)\n",
    "with open(r\"Your file path to the data file...\\scaler_y.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler_y, f)\n",
    "\n",
    "# === Define Improved Autoencoder ===\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, bottleneck_dim=300, output_dim=None):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        output_dim = output_dim or input_dim\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, bottleneck_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(bottleneck_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# === Training setup ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Autoencoder(input_dim=X_train.shape[1], output_dim=Y_train.shape[1]).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "criterion = nn.SmoothL1Loss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5, verbose=True)\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(X_train).float(), torch.tensor(Y_train).float())\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# === Training loop ===\n",
    "epochs = 300\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    scheduler.step(avg_loss)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.6f}\")\n",
    "\n",
    "# === Save model ===\n",
    "save_path = r\"Your file path to the data file...\\autoencoder_model.pth\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Model saved to: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b99672c-9f99-4b35-b760-a6c87e9df426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the test dataset\n",
    "df = pd.read_csv(r\"Your file path to the data file...\")\n",
    "\n",
    "# Exclude the 'geology_id' column from interpolation\n",
    "columns_to_interpolate = df.columns[df.columns != 'geology_id']\n",
    "\n",
    "# Apply linear interpolation across columns (axis=1)\n",
    "df[columns_to_interpolate] = df[columns_to_interpolate].interpolate(\n",
    "    method='linear', axis=1, limit_direction='both'\n",
    ")\n",
    "\n",
    "# Fallback: Fill any remaining NaNs with column means\n",
    "df[columns_to_interpolate] = df[columns_to_interpolate].fillna(df[columns_to_interpolate].mean())\n",
    "\n",
    "# Define the path to save the new CSV\n",
    "save_folder = r\"Your file path to the data file...\"\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "save_path = os.path.join(save_folder, \"filled_test.csv\")\n",
    "\n",
    "# Save the filled DataFrame\n",
    "df.to_csv(save_path, index=False)\n",
    "\n",
    "# Optionally print the first few rows\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a35eec-9513-4fd6-ad49-6f184debebb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "\n",
    "# === Define Improved Autoencoder (matches training) ===\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, bottleneck_dim=300, output_dim=None):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        output_dim = output_dim or input_dim\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, bottleneck_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(bottleneck_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# === Paths ===\n",
    "model_path = r\"Your file path to the data file...\\autoencoder_model.pth\"\n",
    "target_cols_path = r\"Your file path to the data file...\\target_cols.pkl\"\n",
    "scaler_x_path = r\"Your file path to the data file...\\scaler_x.pkl\"\n",
    "scaler_y_path = r\"Your file path to the data file...\\scaler_y.pkl\"\n",
    "test_filled_path = r\"Your file path to the data file...\\filled_test.csv\"\n",
    "submission_path = r\"Your file path to the data file...\\submission.csv\"\n",
    "\n",
    "# === Load test data ===\n",
    "df_test = pd.read_csv(test_filled_path)\n",
    "geology_ids = df_test[\"geology_id\"].reset_index(drop=True)\n",
    "input_cols = [str(i) for i in range(-299, 1) if str(i) in df_test.columns]\n",
    "\n",
    "# === Load saved column list and scalers ===\n",
    "with open(target_cols_path, \"rb\") as f:\n",
    "    target_cols = pickle.load(f)\n",
    "\n",
    "with open(scaler_x_path, \"rb\") as f:\n",
    "    scaler_x = pickle.load(f)\n",
    "\n",
    "with open(scaler_y_path, \"rb\") as f:\n",
    "    scaler_y = pickle.load(f)\n",
    "\n",
    "# === Normalize input using same scaler from training ===\n",
    "X_test = df_test[input_cols].values.astype('float32')\n",
    "X_test_scaled = scaler_x.transform(X_test)\n",
    "\n",
    "# === Model setup ===\n",
    "input_dim = len(input_cols)\n",
    "output_dim = len(target_cols)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = Autoencoder(input_dim=input_dim, output_dim=output_dim).to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# === Predict reconstructed output ===\n",
    "X_test_tensor = torch.tensor(X_test_scaled).to(device)\n",
    "with torch.no_grad():\n",
    "    reconstructed = model(X_test_tensor).cpu().numpy()\n",
    "\n",
    "# === Inverse transform predictions to original scale ===\n",
    "reconstructed_original = scaler_y.inverse_transform(reconstructed)\n",
    "\n",
    "# === Create submission dataframe ===\n",
    "df_reconstructed = pd.DataFrame(reconstructed_original, columns=target_cols)\n",
    "df_submission = pd.concat([geology_ids, df_reconstructed], axis=1)\n",
    "\n",
    "# === Save submission ===\n",
    "df_submission.to_csv(submission_path, index=False)\n",
    "print(f\"Submission file saved to: {submission_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
